{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9A-T0mAW2C0"
      },
      "source": [
        "# Combined Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zUcRgWRYus1"
      },
      "source": [
        "first do the experiment1 ,the combined data, and then split it into 5 folds, then organize them into five pairs of training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_47zy4mY3i_",
        "outputId": "ba173f99-1d48-4f8a-86ec-26b86e1985b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and test data for 5 folds with seed 42 have been generated and saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load the combined data\n",
        "combined_data_42 = pd.read_csv('combined_data_42.csv')\n",
        "\n",
        "# Initialize KFold with 5 splits\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create storage for training and test data splits\n",
        "folds = []\n",
        "\n",
        "# Split the data into 5 folds\n",
        "for train_index, test_index in kf.split(combined_data_42):\n",
        "    train_data = combined_data_42.iloc[train_index]\n",
        "    test_data = combined_data_42.iloc[test_index]\n",
        "    folds.append((train_data, test_data))\n",
        "\n",
        "# Save each training and test group as separate files\n",
        "for i, (train_data, test_data) in enumerate(folds):\n",
        "    train_data.to_csv(f'combined_train_42_fold{i+1}.csv', index=False)\n",
        "    test_data.to_csv(f'combined_test_42_fold{i+1}.csv', index=False)\n",
        "\n",
        "print(\"Training and test data for 5 folds with seed 42 have been generated and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bip-xt9cZv7Y"
      },
      "source": [
        "do the feature selection for each group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462MZV4CE0Kf",
        "outputId": "39fd8d8a-adbe-45d5-8c80-2bbc85e4477d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 features for fold 1 with seed 42 saved.\n",
            "Top 25 features for fold 2 with seed 42 saved.\n",
            "Top 25 features for fold 3 with seed 42 saved.\n",
            "Top 25 features for fold 4 with seed 42 saved.\n",
            "Top 25 features for fold 5 with seed 42 saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Function to perform feature selection\n",
        "def feature_selection(train_data, target_column, preselected_features, k=25):\n",
        "    # Separate features and target\n",
        "    X = train_data.drop(columns=[target_column, 'TIMESTAMP'], errors='ignore')  # Drop TIMESTAMP\n",
        "    y = train_data[target_column]\n",
        "\n",
        "    # Train a RandomForestRegressor model\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=100)\n",
        "    rf_model.fit(X, y)\n",
        "\n",
        "    # Get feature names and their importance scores\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Combine feature names and scores into a DataFrame\n",
        "    selected_features = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Select the preselected features (label, T_out, RH_out)\n",
        "    preselected_df = selected_features[selected_features['Feature'].isin(preselected_features)]\n",
        "\n",
        "    # Select the remaining top features after excluding preselected ones\n",
        "    remaining_features = selected_features[~selected_features['Feature'].isin(preselected_features)]\n",
        "    top_remaining_features = remaining_features.head(k - len(preselected_features))\n",
        "\n",
        "    # Combine preselected features with the top remaining features\n",
        "    final_features = pd.concat([preselected_df, top_remaining_features])\n",
        "\n",
        "    return final_features\n",
        "\n",
        "# Perform feature selection for each fold\n",
        "target_column = 'WH_RTU_Total'\n",
        "preselected_features = ['label', 'T_out', 'RH_out']\n",
        "\n",
        "for i in range(1, 6):\n",
        "    train_data = pd.read_csv(f'combined_train_42_fold{i}.csv')\n",
        "\n",
        "    # Perform feature selection\n",
        "    top_features = feature_selection(train_data, target_column, preselected_features, k=25)\n",
        "\n",
        "    # Save top features\n",
        "    top_features.to_csv(f'combined_train_42_fold{i}_top25_features.csv', index=False)\n",
        "\n",
        "    print(f\"Top 25 features for fold {i} with seed 42 saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSekzqyd9Ocl"
      },
      "source": [
        "then i need to select the related data from both the training and test set in the five groups based on the feature selected in former section(which is the training data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv74sjYB9dw4",
        "outputId": "cc9702d3-b986-49cb-b321-d4b27fddb546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered train data for fold 1 with seed 42 saved.\n",
            "Filtered test data, heating data, and cooling data for fold 1 with seed 42 saved.\n",
            "Filtered train data for fold 2 with seed 42 saved.\n",
            "Filtered test data, heating data, and cooling data for fold 2 with seed 42 saved.\n",
            "Filtered train data for fold 3 with seed 42 saved.\n",
            "Filtered test data, heating data, and cooling data for fold 3 with seed 42 saved.\n",
            "Filtered train data for fold 4 with seed 42 saved.\n",
            "Filtered test data, heating data, and cooling data for fold 4 with seed 42 saved.\n",
            "Filtered train data for fold 5 with seed 42 saved.\n",
            "Filtered test data, heating data, and cooling data for fold 5 with seed 42 saved.\n"
          ]
        }
      ],
      "source": [
        "# Target variable\n",
        "target_column = 'WH_RTU_Total'\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load the original training and test datasets for the current fold\n",
        "    train_data = pd.read_csv(f'combined_train_42_fold{i}.csv')\n",
        "    test_data = pd.read_csv(f'combined_test_42_fold{i}.csv')\n",
        "\n",
        "    # Load the top 25 features selected from the current fold\n",
        "    top_features = pd.read_csv(f'combined_train_42_fold{i}_top25_features.csv')['Feature'].tolist()\n",
        "\n",
        "    # Ensure the target column is included in the selected features\n",
        "    train_columns = top_features + [target_column]  # Train data includes target\n",
        "    test_columns = top_features + [target_column]  # Test data includes target\n",
        "\n",
        "    # Filter the training data\n",
        "    filtered_train_data = train_data[train_columns]\n",
        "    filtered_train_data.to_csv(f'fold_42_{i}_filtered_train_data.csv', index=False)\n",
        "    print(f\"Filtered train data for fold {i} with seed 42 saved.\")\n",
        "\n",
        "    # Filter the test data\n",
        "    filtered_test_data = test_data[test_columns]\n",
        "\n",
        "    # Split the test data into heating and cooling datasets\n",
        "    test_data_heating = filtered_test_data[filtered_test_data['label'] == 1]\n",
        "    test_data_cooling = filtered_test_data[filtered_test_data['label'] == 0]\n",
        "\n",
        "    # Save all datasets\n",
        "    filtered_test_data.to_csv(f'fold_42_{i}_filtered_test_data.csv', index=False)\n",
        "    test_data_heating.to_csv(f'fold_42_{i}_filtered_test_data_heating.csv', index=False)\n",
        "    test_data_cooling.to_csv(f'fold_42_{i}_filtered_test_data_cooling.csv', index=False)\n",
        "    print(f\"Filtered test data, heating data, and cooling data for fold {i} with seed 42 saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h53709sH-kya"
      },
      "source": [
        "generate the mae and R2 value for the four models:linear regression, random forest ,xgboost and stacking ensemble regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2sogHDs_Afe",
        "outputId": "da9a6328-bd85-4bab-cfc6-12f100ad485c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Fold 1, Model Linear Regression, Dataset Combined, MAE: 4.4129, R²: 0.6871\n",
            "Fold 1, Model Linear Regression, Dataset Cooling, MAE: 4.8859, R²: 0.4429\n",
            "Fold 1, Model Linear Regression, Dataset Heating, MAE: 3.8922, R²: 0.3031\n",
            "Fold 1, Model Random Forest, Dataset Combined, MAE: 2.8386, R²: 0.8362\n",
            "Fold 1, Model Random Forest, Dataset Cooling, MAE: 2.5654, R²: 0.7507\n",
            "Fold 1, Model Random Forest, Dataset Heating, MAE: 3.1393, R²: 0.5090\n",
            "Fold 1, Model XGBoost, Dataset Combined, MAE: 2.7947, R²: 0.8374\n",
            "Fold 1, Model XGBoost, Dataset Cooling, MAE: 2.4817, R²: 0.7626\n",
            "Fold 1, Model XGBoost, Dataset Heating, MAE: 3.1391, R²: 0.4830\n",
            "Fold 1, Model Stacking Regressor, Dataset Combined, MAE: 2.9883, R²: 0.8367\n",
            "Fold 1, Model Stacking Regressor, Dataset Cooling, MAE: 2.7672, R²: 0.7542\n",
            "Fold 1, Model Stacking Regressor, Dataset Heating, MAE: 3.2317, R²: 0.5026\n",
            "Fold 2, Model Linear Regression, Dataset Combined, MAE: 4.5668, R²: 0.6846\n",
            "Fold 2, Model Linear Regression, Dataset Cooling, MAE: 5.2644, R²: 0.3640\n",
            "Fold 2, Model Linear Regression, Dataset Heating, MAE: 3.8620, R²: 0.3353\n",
            "Fold 2, Model Random Forest, Dataset Combined, MAE: 2.7771, R²: 0.8465\n",
            "Fold 2, Model Random Forest, Dataset Cooling, MAE: 2.3998, R²: 0.7476\n",
            "Fold 2, Model Random Forest, Dataset Heating, MAE: 3.1583, R²: 0.5356\n",
            "Fold 2, Model XGBoost, Dataset Combined, MAE: 2.8877, R²: 0.8397\n",
            "Fold 2, Model XGBoost, Dataset Cooling, MAE: 2.5377, R²: 0.7460\n",
            "Fold 2, Model XGBoost, Dataset Heating, MAE: 3.2413, R²: 0.4916\n",
            "Fold 2, Model Stacking Regressor, Dataset Combined, MAE: 2.8391, R²: 0.8502\n",
            "Fold 2, Model Stacking Regressor, Dataset Cooling, MAE: 2.4701, R²: 0.7563\n",
            "Fold 2, Model Stacking Regressor, Dataset Heating, MAE: 3.2119, R²: 0.5408\n",
            "Fold 3, Model Linear Regression, Dataset Combined, MAE: 4.6170, R²: 0.6434\n",
            "Fold 3, Model Linear Regression, Dataset Cooling, MAE: 5.2482, R²: 0.3998\n",
            "Fold 3, Model Linear Regression, Dataset Heating, MAE: 4.0015, R²: 0.3066\n",
            "Fold 3, Model Random Forest, Dataset Combined, MAE: 2.7057, R²: 0.8504\n",
            "Fold 3, Model Random Forest, Dataset Cooling, MAE: 2.1899, R²: 0.8143\n",
            "Fold 3, Model Random Forest, Dataset Heating, MAE: 3.2086, R²: 0.5379\n",
            "Fold 3, Model XGBoost, Dataset Combined, MAE: 2.8781, R²: 0.8303\n",
            "Fold 3, Model XGBoost, Dataset Cooling, MAE: 2.3881, R²: 0.7871\n",
            "Fold 3, Model XGBoost, Dataset Heating, MAE: 3.3559, R²: 0.4810\n",
            "Fold 3, Model Stacking Regressor, Dataset Combined, MAE: 2.8444, R²: 0.8477\n",
            "Fold 3, Model Stacking Regressor, Dataset Cooling, MAE: 2.3888, R²: 0.8088\n",
            "Fold 3, Model Stacking Regressor, Dataset Heating, MAE: 3.2886, R²: 0.5350\n",
            "Fold 4, Model Linear Regression, Dataset Combined, MAE: 4.3143, R²: 0.6895\n",
            "Fold 4, Model Linear Regression, Dataset Cooling, MAE: 4.6945, R²: 0.4389\n",
            "Fold 4, Model Linear Regression, Dataset Heating, MAE: 3.9398, R²: 0.3148\n",
            "Fold 4, Model Random Forest, Dataset Combined, MAE: 2.7619, R²: 0.8464\n",
            "Fold 4, Model Random Forest, Dataset Cooling, MAE: 2.2514, R²: 0.7816\n",
            "Fold 4, Model Random Forest, Dataset Heating, MAE: 3.2648, R²: 0.5259\n",
            "Fold 4, Model XGBoost, Dataset Combined, MAE: 2.9054, R²: 0.8351\n",
            "Fold 4, Model XGBoost, Dataset Cooling, MAE: 2.5054, R²: 0.7640\n",
            "Fold 4, Model XGBoost, Dataset Heating, MAE: 3.2994, R²: 0.4943\n",
            "Fold 4, Model Stacking Regressor, Dataset Combined, MAE: 2.8120, R²: 0.8465\n",
            "Fold 4, Model Stacking Regressor, Dataset Cooling, MAE: 2.3283, R²: 0.7824\n",
            "Fold 4, Model Stacking Regressor, Dataset Heating, MAE: 3.2884, R²: 0.5246\n",
            "Fold 5, Model Linear Regression, Dataset Combined, MAE: 4.6298, R²: 0.6778\n",
            "Fold 5, Model Linear Regression, Dataset Cooling, MAE: 5.1920, R²: 0.4240\n",
            "Fold 5, Model Linear Regression, Dataset Heating, MAE: 4.1034, R²: 0.2479\n",
            "Fold 5, Model Random Forest, Dataset Combined, MAE: 2.7734, R²: 0.8493\n",
            "Fold 5, Model Random Forest, Dataset Cooling, MAE: 2.1862, R²: 0.8021\n",
            "Fold 5, Model Random Forest, Dataset Heating, MAE: 3.3233, R²: 0.4826\n",
            "Fold 5, Model XGBoost, Dataset Combined, MAE: 2.9051, R²: 0.8351\n",
            "Fold 5, Model XGBoost, Dataset Cooling, MAE: 2.3687, R²: 0.7828\n",
            "Fold 5, Model XGBoost, Dataset Heating, MAE: 3.4073, R²: 0.4351\n",
            "Fold 5, Model Stacking Regressor, Dataset Combined, MAE: 2.9103, R²: 0.8481\n",
            "Fold 5, Model Stacking Regressor, Dataset Cooling, MAE: 2.4045, R²: 0.7995\n",
            "Fold 5, Model Stacking Regressor, Dataset Heating, MAE: 3.3840, R²: 0.4807\n",
            "\n",
            "Summary Results for All Models:\n",
            "                                  MAE        R²\n",
            "Model              Dataset                     \n",
            "Linear Regression  Combined  4.508148  0.676483\n",
            "                   Cooling   5.057008  0.413907\n",
            "                   Heating   3.959775  0.301534\n",
            "Random Forest      Combined  2.771348  0.845768\n",
            "                   Cooling   2.318511  0.779259\n",
            "                   Heating   3.218885  0.518213\n",
            "Stacking Regressor Combined  2.878824  0.845856\n",
            "                   Cooling   2.471770  0.780246\n",
            "                   Heating   3.280926  0.516735\n",
            "XGBoost            Combined  2.874200  0.835513\n",
            "                   Cooling   2.456325  0.768520\n",
            "                   Heating   3.288629  0.477001\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results = []\n",
        "\n",
        "\n",
        "# Loop through each fold\n",
        "for i in range(1, 6):\n",
        "    # Load filtered training and test datasets for the current fold\n",
        "    train_data = pd.read_csv(f'fold_42_{i}_filtered_train_data.csv')\n",
        "    test_data_combined = pd.read_csv(f'fold_42_{i}_filtered_test_data.csv')\n",
        "    test_data_cooling = pd.read_csv(f'fold_42_{i}_filtered_test_data_cooling.csv')\n",
        "    test_data_heating = pd.read_csv(f'fold_42_{i}_filtered_test_data_heating.csv')\n",
        "\n",
        "    # Separate features and target for training data\n",
        "    target_column = 'WH_RTU_Total'\n",
        "    X_train = train_data.drop(columns=[target_column])\n",
        "    y_train = train_data[target_column]\n",
        "\n",
        "    # Define models with default parameters\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(random_state=100),\n",
        "        'XGBoost': XGBRegressor(random_state=100),\n",
        "        'Stacking Regressor': StackingRegressor(\n",
        "            estimators=[\n",
        "                ('rf', RandomForestRegressor(random_state=100)),  # Random Forest\n",
        "                ('gb', GradientBoostingRegressor(random_state=100))  # Gradient Boosting\n",
        "            ],\n",
        "            final_estimator=LinearRegression()  # Linear Regression as the meta-learner\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Function to evaluate models on a given dataset\n",
        "    def evaluate_model(model, X_train, y_train, X_test, y_test, dataset_type):\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        return mae, r2\n",
        "\n",
        "    # Evaluate models on combined, cooling, and heating datasets\n",
        "    for model_name, model in models.items():\n",
        "        for dataset_type, test_data in [('Combined', test_data_combined),\n",
        "                                        ('Cooling', test_data_cooling),\n",
        "                                        ('Heating', test_data_heating)]:\n",
        "            # Separate features and target for the current test dataset\n",
        "            X_test = test_data.drop(columns=[target_column])\n",
        "            y_test = test_data[target_column]\n",
        "\n",
        "            # Calculate MAE and R²\n",
        "            mae, r2 = evaluate_model(model, X_train, y_train, X_test, y_test, dataset_type)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'Fold': i,\n",
        "                'Seed': 42,  # Add the seed column\n",
        "                'Model': model_name,\n",
        "                'Dataset': dataset_type,\n",
        "                'MAE': mae,\n",
        "                'R²': r2,\n",
        "                'Experiment': 1  # Add the experiment column\n",
        "            })\n",
        "\n",
        "            print(f\"Fold {i}, Model {model_name}, Dataset {dataset_type}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(f'model_results_42_mae_r2_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for All Models:\")\n",
        "summary = results_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).sort_values(by=['Model', 'Dataset'])\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D57gmj6zG13"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjRib9g8gyPY"
      },
      "source": [
        "### Random Forest Hyperparameter Tuning python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COH3JRgRSDfJ",
        "outputId": "d96079cb-dac2-4a40-f3d4-102c50caf204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Model Random Forest, Dataset Combined, MAE: 2.8406, R²: 0.8350, Best Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 150}\n",
            "Fold 1, Model Random Forest, Dataset Cooling, MAE: 2.5471, R²: 0.7490, Best Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 150}\n",
            "Fold 1, Model Random Forest, Dataset Heating, MAE: 3.1636, R²: 0.5054, Best Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 150}\n",
            "Fold 2, Model Random Forest, Dataset Combined, MAE: 2.7867, R²: 0.8436, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 2, Model Random Forest, Dataset Cooling, MAE: 2.4041, R²: 0.7353, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 2, Model Random Forest, Dataset Heating, MAE: 3.1731, R²: 0.5455, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 3, Model Random Forest, Dataset Combined, MAE: 2.7254, R²: 0.8499, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Fold 3, Model Random Forest, Dataset Cooling, MAE: 2.2100, R²: 0.8093, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Fold 3, Model Random Forest, Dataset Heating, MAE: 3.2279, R²: 0.5471, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Fold 4, Model Random Forest, Dataset Combined, MAE: 2.7609, R²: 0.8465, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 4, Model Random Forest, Dataset Cooling, MAE: 2.2896, R²: 0.7727, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 4, Model Random Forest, Dataset Heating, MAE: 3.2252, R²: 0.5467, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 5, Model Random Forest, Dataset Combined, MAE: 2.7695, R²: 0.8506, Best Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 5, Model Random Forest, Dataset Cooling, MAE: 2.1733, R²: 0.8046, Best Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 5, Model Random Forest, Dataset Heating, MAE: 3.3279, R²: 0.4851, Best Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "\n",
            "Summary Results for Random Forest:\n",
            "           Model   Dataset       MAE        R²\n",
            "0  Random Forest  Combined  2.776617  0.845118\n",
            "1  Random Forest   Cooling  2.324826  0.774195\n",
            "2  Random Forest   Heating  3.223532  0.525964\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_rf = []\n",
        "\n",
        "\n",
        "\n",
        "# Define hyperparameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold_42_{i}_filtered_train_data.csv')\n",
        "    test_data_combined = pd.read_csv(f'fold_42_{i}_filtered_test_data.csv')\n",
        "    test_data_cooling = pd.read_csv(f'fold_42_{i}_filtered_test_data_cooling.csv')\n",
        "    test_data_heating = pd.read_csv(f'fold_42_{i}_filtered_test_data_heating.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Initialize Random Forest and perform GridSearchCV\n",
        "    rf = RandomForestRegressor(random_state=100)\n",
        "    grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_rf = grid_search_rf.best_estimator_\n",
        "    best_params = grid_search_rf.best_params_\n",
        "\n",
        "    # Evaluate on Combined, Cooling, and Heating datasets\n",
        "    for dataset_type, test_data in [('Combined', test_data_combined),\n",
        "                                    ('Cooling', test_data_cooling),\n",
        "                                    ('Heating', test_data_heating)]:\n",
        "        # Split test features and target\n",
        "        X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "        y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "        # Predict and calculate metrics\n",
        "        y_pred = best_rf.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results_rf.append({\n",
        "            'Fold': i,\n",
        "            'Seed': 42,  # Add the seed column\n",
        "            'Model': 'Random Forest',\n",
        "            'Dataset': dataset_type,\n",
        "            'MAE': mae,\n",
        "            'R²': r2,\n",
        "            'Best Params': best_params,\n",
        "            'Experiment': 1\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {i}, Model Random Forest, Dataset {dataset_type}, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_rf_df = pd.DataFrame(results_rf)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_rf_df.to_csv(f'random_forest_42_combined_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for Random Forest:\")\n",
        "summary_rf = results_rf_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_rf)\n",
        "\n",
        "# Save summary to CSV 这个section里面有没有inner seed呢？\n",
        "summary_rf.to_csv(f'random_forest_42_combined_hyperparameter_results_summary.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qyjZwoTh0BE"
      },
      "source": [
        "### random forest feature importance generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLUGoQ-5h5X4",
        "outputId": "57811461-7621-457a-8f0b-c30ecef27f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold_42_{fold}_filtered_train_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model for the current fold\n",
        "    results_rf_df = pd.read_csv(f'random_forest_42_combined_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_rf_df.loc[(results_rf_df['Fold'] == fold) & (results_rf_df['Dataset'] == 'Combined'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "    best_rf = RandomForestRegressor(random_state=100, **best_params_dict)\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importances = best_rf.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances,\n",
        "        'Fold': fold,\n",
        "        'Seed': 42,  # Add seed column\n",
        "        'Model': 'Random Forest',\n",
        "        'Experiment': 1 # Add model column\n",
        "    })\n",
        "    feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine and save feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "feature_importance_combined.to_csv(f'random_forest_42_combined_feature_importance.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 has been successfully saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6xVkVnusUw"
      },
      "source": [
        "### generate residual ,predict, label, tout and rh out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzWEI7HQWOlq",
        "outputId": "481a7cdb-ca55-4ffb-f5e9-4ad1a15a949a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1, dataset cooling.\n",
            "Processed fold 1, dataset heating.\n",
            "Processed fold 2, dataset cooling.\n",
            "Processed fold 2, dataset heating.\n",
            "Processed fold 3, dataset cooling.\n",
            "Processed fold 3, dataset heating.\n",
            "Processed fold 4, dataset cooling.\n",
            "Processed fold 4, dataset heating.\n",
            "Processed fold 5, dataset cooling.\n",
            "Processed fold 5, dataset heating.\n",
            "All folds processed and merged data saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Fixed seed value\n",
        "\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold_42_{fold}_filtered_train_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    best_params_df = pd.read_csv('random_forest_42_combined_hyperparameter_results_detailed.csv')\n",
        "    best_params = best_params_df.loc[(best_params_df['Fold'] == fold) & (best_params_df['Dataset'] == 'Combined'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train Random Forest model\n",
        "    best_rf = RandomForestRegressor(random_state=100, **best_params_dict)\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Loop through Cooling and Heating datasets\n",
        "    for dataset_type in ['cooling', 'heating']:\n",
        "        # Load test data\n",
        "        test_data = pd.read_csv(f'fold_42_{fold}_filtered_test_data_{dataset_type}.csv')\n",
        "\n",
        "        # Split features and target\n",
        "        X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "        y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "        # Predict and calculate residuals\n",
        "        y_pred = best_rf.predict(X_test)\n",
        "        test_data['Predicted'] = y_pred\n",
        "        test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "        # Select and rename required columns\n",
        "        if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "            selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'label']].rename(columns={\n",
        "                'WH_RTU_Total': 'Actual'\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(f\"'T_out' or 'RH_out' is missing in test data for fold {fold} and dataset {dataset_type}\")\n",
        "\n",
        "        # Add fold, seed, and model columns\n",
        "        selected_data['Fold'] = fold\n",
        "        selected_data['Seed'] = 42\n",
        "        selected_data['Model'] = 'Random Forest'\n",
        "        selected_data['Experiment'] = 1\n",
        "\n",
        "        # Append to final merged data\n",
        "        merged_data.append(selected_data)\n",
        "        print(f\"Processed fold {fold}, dataset {dataset_type}.\")\n",
        "\n",
        "# Combine all folds and datasets into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "# Save the merged data\n",
        "merged_final_data.to_csv(f'random_forest_42_combined_residual.csv', index=False)\n",
        "\n",
        "print(\"All folds processed and merged data saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH1AKQHzzUfc"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac1tUI3UjOMF"
      },
      "source": [
        "### XGBoost Hyperparameter Tuning python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeWM0_T0jNUF",
        "outputId": "1d8749ca-7703-4b72-c009-09b88c476fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n",
            "Fold 1, Model XGBoost, Dataset Combined, MAE: 2.7525, R²: 0.8469, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 1, Model XGBoost, Dataset Cooling, MAE: 2.3866, R²: 0.7826, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 1, Model XGBoost, Dataset Heating, MAE: 3.1553, R²: 0.4951, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 2, Model XGBoost, Dataset Combined, MAE: 2.7602, R²: 0.8493, Best Params: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 50, 'subsample': 0.8}\n",
            "Fold 2, Model XGBoost, Dataset Cooling, MAE: 2.3681, R²: 0.7570, Best Params: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 50, 'subsample': 0.8}\n",
            "Fold 2, Model XGBoost, Dataset Heating, MAE: 3.1563, R²: 0.5324, Best Params: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 50, 'subsample': 0.8}\n",
            "Fold 3, Model XGBoost, Dataset Combined, MAE: 2.7552, R²: 0.8508, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 1.0}\n",
            "Fold 3, Model XGBoost, Dataset Cooling, MAE: 2.3065, R²: 0.8106, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 1.0}\n",
            "Fold 3, Model XGBoost, Dataset Heating, MAE: 3.1927, R²: 0.5499, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 1.0}\n",
            "Fold 4, Model XGBoost, Dataset Combined, MAE: 2.7714, R²: 0.8483, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 1.0}\n",
            "Fold 4, Model XGBoost, Dataset Cooling, MAE: 2.3274, R²: 0.7826, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 1.0}\n",
            "Fold 4, Model XGBoost, Dataset Heating, MAE: 3.2088, R²: 0.5353, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 1.0}\n",
            "Fold 5, Model XGBoost, Dataset Combined, MAE: 2.8278, R²: 0.8432, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 1.0}\n",
            "Fold 5, Model XGBoost, Dataset Cooling, MAE: 2.2730, R²: 0.7929, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 1.0}\n",
            "Fold 5, Model XGBoost, Dataset Heating, MAE: 3.3474, R²: 0.4643, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 1.0}\n",
            "\n",
            "Summary Results for XGBoost:\n",
            "     Model   Dataset       MAE        R²\n",
            "0  XGBoost  Combined  2.773434  0.847705\n",
            "1  XGBoost   Cooling  2.332332  0.785145\n",
            "2  XGBoost   Heating  3.212080  0.515408\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost\n",
        "!pip install scikit-learn==1.0.2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_xgb = []\n",
        "\n",
        "\n",
        "\n",
        "# Define hyperparameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold_42_{i}_filtered_train_data.csv')\n",
        "    test_data_combined = pd.read_csv(f'fold_42_{i}_filtered_test_data.csv')\n",
        "    test_data_cooling = pd.read_csv(f'fold_42_{i}_filtered_test_data_cooling.csv')\n",
        "    test_data_heating = pd.read_csv(f'fold_42_{i}_filtered_test_data_heating.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Initialize XGBoost and perform GridSearchCV\n",
        "    xgb = XGBRegressor(random_state=100)\n",
        "    grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_xgb = grid_search_xgb.best_estimator_\n",
        "    best_params = grid_search_xgb.best_params_\n",
        "\n",
        "    # Evaluate on Combined, Cooling, and Heating datasets\n",
        "    for dataset_type, test_data in [('Combined', test_data_combined),\n",
        "                                    ('Cooling', test_data_cooling),\n",
        "                                    ('Heating', test_data_heating)]:\n",
        "        # Split test features and target\n",
        "        X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "        y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "        # Predict and calculate metrics\n",
        "        y_pred = best_xgb.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results_xgb.append({\n",
        "            'Fold': i,\n",
        "            'Seed': 42,\n",
        "            'Model': 'XGBoost',\n",
        "            'Dataset': dataset_type,\n",
        "            'MAE': mae,\n",
        "            'R²': r2,\n",
        "            'Best Params': best_params,\n",
        "            'Experiment': 1\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {i}, Model XGBoost, Dataset {dataset_type}, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_xgb_df = pd.DataFrame(results_xgb)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_xgb_df.to_csv(f'xgboost_42_combined_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for XGBoost:\")\n",
        "summary_xgb = results_xgb_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_xgb)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_xgb.to_csv(f'xgboost_42_combined_hyperparameter_results_summary.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEs4F6Fdw28P"
      },
      "source": [
        "### feature importance for xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jafrscDjw6vM",
        "outputId": "8d587263-8ee6-4a30-f9ac-9dea877d6d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold_42_{fold}_filtered_train_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Extract the best parameters from the results file\n",
        "    results_xgb_df = pd.read_csv(f'xgboost_42_combined_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_xgb_df.loc[\n",
        "        (results_xgb_df['Fold'] == fold) & (results_xgb_df['Dataset'] == 'Combined'),\n",
        "        'Best Params'\n",
        "    ].iloc[0]\n",
        "    best_params_dict = eval(best_params)  # Parse string into dictionary\n",
        "\n",
        "    # Initialize and train the model with the best parameters\n",
        "    best_xgb = XGBRegressor(random_state=100, **best_params_dict)\n",
        "    best_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importances = best_xgb.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances,\n",
        "        'Fold': fold,\n",
        "        'Seed': 42,  # Add seed column\n",
        "        'Model': 'XGBoost',\n",
        "        'Experiment': 1  # Add model column\n",
        "    })\n",
        "    feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine all feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "\n",
        "# Save the feature importance results to a file\n",
        "feature_importance_combined.to_csv(f'xgboost_42_combined_feature_importance.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 has been successfully saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4_dQgOGC6LQ"
      },
      "source": [
        "### generate residual，predict，label，tout and rh out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW4I3AgpDCnf",
        "outputId": "4f6143e0-5ea4-498e-910a-1a557e2f2350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1, dataset cooling.\n",
            "Processed fold 1, dataset heating.\n",
            "Processed fold 2, dataset cooling.\n",
            "Processed fold 2, dataset heating.\n",
            "Processed fold 3, dataset cooling.\n",
            "Processed fold 3, dataset heating.\n",
            "Processed fold 4, dataset cooling.\n",
            "Processed fold 4, dataset heating.\n",
            "Processed fold 5, dataset cooling.\n",
            "Processed fold 5, dataset heating.\n",
            "All folds processed and merged data saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Fixed seed value\n",
        "\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold_42_{fold}_filtered_train_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    best_params_df = pd.read_csv('xgboost_42_combined_hyperparameter_results_detailed.csv')\n",
        "    best_params = best_params_df.loc[(best_params_df['Fold'] == fold) & (best_params_df['Dataset'] == 'Combined'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train XGBoost model\n",
        "    best_xgb = XGBRegressor(random_state=100, **best_params_dict)\n",
        "    best_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Loop through Cooling and Heating datasets\n",
        "    for dataset_type in ['cooling', 'heating']:\n",
        "        # Load test data\n",
        "        test_data = pd.read_csv(f'fold_42_{fold}_filtered_test_data_{dataset_type}.csv')\n",
        "\n",
        "        # Split features and target\n",
        "        X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "        y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "        # Predict and calculate residuals\n",
        "        y_pred = best_xgb.predict(X_test)\n",
        "        test_data['Predicted'] = y_pred\n",
        "        test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "        # Select and rename required columns\n",
        "        if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "            selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'label']].rename(columns={\n",
        "                'WH_RTU_Total': 'Actual'\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(f\"'T_out' or 'RH_out' is missing in test data for fold {fold} and dataset {dataset_type}\")\n",
        "\n",
        "        # Add fold, seed, and model columns\n",
        "        selected_data['Fold'] = fold\n",
        "        selected_data['Seed'] = 42\n",
        "        selected_data['Model'] = 'XGBoost'\n",
        "        selected_data['Experiment'] = 1\n",
        "\n",
        "        # Append to final merged data\n",
        "        merged_data.append(selected_data)\n",
        "        print(f\"Processed fold {fold}, dataset {dataset_type}.\")\n",
        "\n",
        "# Combine all folds and datasets into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "# Save the merged data\n",
        "merged_final_data.to_csv(f'xgboost_42_combined_residual.csv', index=False)\n",
        "\n",
        "print(\"All folds processed and merged data saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li5yH4TMzcgE"
      },
      "source": [
        "## Stacking Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJ1JeVBqFYZ"
      },
      "source": [
        "### Stacking Regressor Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOnwFdcAqH_V",
        "outputId": "17fa6cc4-1c08-4f30-98d4-082a66e9bce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Model Stacking Regressor, Dataset Combined, MAE: 3.0183, R²: 0.8374, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 1, Model Stacking Regressor, Dataset Cooling, MAE: 2.7828, R²: 0.7576, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 1, Model Stacking Regressor, Dataset Heating, MAE: 3.2775, R²: 0.4981, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 2, Model Stacking Regressor, Dataset Combined, MAE: 2.7837, R²: 0.8492, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 2, Model Stacking Regressor, Dataset Cooling, MAE: 2.3773, R²: 0.7574, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 2, Model Stacking Regressor, Dataset Heating, MAE: 3.1943, R²: 0.5306, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 3, Model Stacking Regressor, Dataset Combined, MAE: 2.7253, R²: 0.8513, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 3, Model Stacking Regressor, Dataset Cooling, MAE: 2.1823, R²: 0.8164, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 3, Model Stacking Regressor, Dataset Heating, MAE: 3.2547, R²: 0.5380, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 4, Model Stacking Regressor, Dataset Combined, MAE: 2.7280, R²: 0.8505, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
            "Fold 4, Model Stacking Regressor, Dataset Cooling, MAE: 2.2421, R²: 0.7820, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
            "Fold 4, Model Stacking Regressor, Dataset Heating, MAE: 3.2065, R²: 0.5511, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
            "Fold 5, Model Stacking Regressor, Dataset Combined, MAE: 2.7782, R²: 0.8527, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 5, Model Stacking Regressor, Dataset Cooling, MAE: 2.2169, R²: 0.8060, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 5, Model Stacking Regressor, Dataset Heating, MAE: 3.3038, R²: 0.4952, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "\n",
            "Summary Results for Stacking Regressor:\n",
            "                Model   Dataset       MAE        R²\n",
            "0  Stacking Regressor  Combined  2.806688  0.848230\n",
            "1  Stacking Regressor   Cooling  2.360281  0.783877\n",
            "2  Stacking Regressor   Heating  3.247363  0.522605\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_stacking = []\n",
        "\n",
        "\n",
        "# Define hyperparameter grid for Stacking Regressor\n",
        "param_grid_stacking = {\n",
        "    'final_estimator__fit_intercept': [True, False],  # Meta-model parameter (LinearRegression)\n",
        "    'rf__n_estimators': [50, 100],                   # Base model: Random Forest\n",
        "    'rf__max_depth': [10, 20],                       # Base model: Random Forest\n",
        "    'gb__learning_rate': [0.01, 0.1],                # Base model: Gradient Boosting\n",
        "    'gb__max_depth': [3, 6]                          # Base model: Gradient Boosting\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold_42_{i}_filtered_train_data.csv')\n",
        "    test_data_combined = pd.read_csv(f'fold_42_{i}_filtered_test_data.csv')\n",
        "    test_data_cooling = pd.read_csv(f'fold_42_{i}_filtered_test_data_cooling.csv')\n",
        "    test_data_heating = pd.read_csv(f'fold_42_{i}_filtered_test_data_heating.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Define base models and Stacking Regressor\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100)),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100))\n",
        "    ]\n",
        "    meta_model = LinearRegression()\n",
        "    stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "    # Perform GridSearchCV on the Stacking Regressor\n",
        "    grid_search_stacking = GridSearchCV(stacking, param_grid_stacking, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_stacking = grid_search_stacking.best_estimator_\n",
        "    best_params = grid_search_stacking.best_params_\n",
        "\n",
        "    # Evaluate on Combined, Cooling, and Heating datasets\n",
        "    for dataset_type, test_data in [('Combined', test_data_combined),\n",
        "                                    ('Cooling', test_data_cooling),\n",
        "                                    ('Heating', test_data_heating)]:\n",
        "        # Split test features and target\n",
        "        X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "        y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "        # Predict and calculate metrics\n",
        "        y_pred = best_stacking.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results_stacking.append({\n",
        "            'Fold': i,\n",
        "            'Seed': 42,  # Add seed column\n",
        "            'Model': 'Stacking Regressor',\n",
        "            'Dataset': dataset_type,\n",
        "            'MAE': mae,\n",
        "            'R²': r2,\n",
        "            'Best Params': best_params,\n",
        "            'Experiment': 1  # Add model column\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {i}, Model Stacking Regressor, Dataset {dataset_type}, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_stacking_df = pd.DataFrame(results_stacking)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_stacking_df.to_csv('stacking_42_combined_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for Stacking Regressor:\")\n",
        "summary_stacking = results_stacking_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_stacking)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_stacking.to_csv('stacking_42_combined_hyperparameter_results_summary.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_4JmnPXx5gl"
      },
      "source": [
        "### feature importance for combine stacking regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERQYu4Mxx5Gq",
        "outputId": "648c9938-8857-4bd0-c7bc-28241039a90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold_42_{fold}_filtered_train_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Extract the best parameters from the results file\n",
        "    results_stacking_df = pd.read_csv(f'stacking_42_combined_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_stacking_df.loc[\n",
        "        (results_stacking_df['Fold'] == fold) & (results_stacking_df['Dataset'] == 'Combined'),\n",
        "        'Best Params'\n",
        "    ].iloc[0]\n",
        "    best_params_dict = eval(best_params)  # Parse string into dictionary\n",
        "\n",
        "    # Define base models\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100, **{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('rf__')})),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100, **{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('gb__')}))\n",
        "    ]\n",
        "    meta_model = LinearRegression(**{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('final_estimator__')})\n",
        "\n",
        "    # Define Stacking Regressor and fit the model\n",
        "    stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "    stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importances from base models\n",
        "    for model_name, model in stacking.named_estimators_.items():\n",
        "        if hasattr(model, 'feature_importances_'):  # Check if the model supports feature importance\n",
        "            feature_importances = model.feature_importances_\n",
        "            feature_names = X_train.columns\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': feature_importances,\n",
        "                'Model': model_name,\n",
        "                'Fold': fold,\n",
        "                'Seed': 42,\n",
        "                'Experiment': 1  # Add seed column for reference\n",
        "            })\n",
        "            feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine all feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "\n",
        "# Save the feature importance results to a file\n",
        "feature_importance_combined.to_csv(f'stacking_42_combined_feature_importance.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 has been successfully saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqruwTArcjs0"
      },
      "source": [
        "### generate residual ，predict，label，t_out and RH_OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMfd6F22W2HB",
        "outputId": "a8c32a40-7788-49d6-b137-3b967245bbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1, dataset cooling.\n",
            "Processed fold 1, dataset heating.\n",
            "Processed fold 2, dataset cooling.\n",
            "Processed fold 2, dataset heating.\n",
            "Processed fold 3, dataset cooling.\n",
            "Processed fold 3, dataset heating.\n",
            "Processed fold 4, dataset cooling.\n",
            "Processed fold 4, dataset heating.\n",
            "Processed fold 5, dataset cooling.\n",
            "Processed fold 5, dataset heating.\n",
            "All folds processed and merged data saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Load the best model parameters for each fold\n",
        "best_params_df = pd.read_csv('stacking_42_combined_hyperparameter_results_detailed.csv')\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold_42_{fold}_filtered_train_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Get the best parameters for the current fold\n",
        "    best_params = best_params_df.loc[best_params_df['Fold'] == fold, 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Define base models and Stacking Regressor with best parameters\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100,\n",
        "                                     n_estimators=best_params_dict['rf__n_estimators'],\n",
        "                                     max_depth=best_params_dict['rf__max_depth'])),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100,\n",
        "                                         learning_rate=best_params_dict['gb__learning_rate'],\n",
        "                                         max_depth=best_params_dict['gb__max_depth']))\n",
        "    ]\n",
        "    meta_model = LinearRegression(fit_intercept=best_params_dict['final_estimator__fit_intercept'])\n",
        "    best_stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "    # Train the Stacking Regressor\n",
        "    best_stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Loop through Cooling and Heating datasets\n",
        "    for dataset_type in ['cooling', 'heating']:\n",
        "        # Load test data\n",
        "        test_data = pd.read_csv(f'fold_42_{fold}_filtered_test_data_{dataset_type}.csv')\n",
        "\n",
        "        # Split features and target\n",
        "        X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "        y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "        # Predict and calculate residuals\n",
        "        y_pred = best_stacking.predict(X_test)\n",
        "        test_data['Predicted'] = y_pred\n",
        "        test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "        # Select and rename required columns\n",
        "        if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "            selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'label']].rename(columns={\n",
        "                'WH_RTU_Total': 'Actual'\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(f\"'T_out' or 'RH_out' is missing in test data for fold {fold} and dataset {dataset_type}\")\n",
        "\n",
        "        # Add fold, seed, and model columns\n",
        "        selected_data['Fold'] = fold\n",
        "        selected_data['Seed'] = 42\n",
        "        selected_data['Model'] = 'StackingRegressor'\n",
        "        selected_data['Experiment'] = 1\n",
        "\n",
        "        # Append to final merged data\n",
        "        merged_data.append(selected_data)\n",
        "        print(f\"Processed fold {fold}, dataset {dataset_type}.\")\n",
        "\n",
        "# Combine all folds and datasets into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "# Save the merged data\n",
        "merged_final_data.to_csv(f'stacking_42_combined_residual.csv', index=False)\n",
        "\n",
        "print(\"All folds processed and merged data saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4igMB904NPx9"
      },
      "source": [
        "# Cooling Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P2T4w25Mcz6"
      },
      "source": [
        "do the same model performance for cooling data, first divide the cooling data into 5 folds and generate differet training and testing groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8qgHS9IMnUE",
        "outputId": "5ae98eb9-deed-4050-ee90-07afa15bf8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and test data for 5 folds with seed 42 have been generated and saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load the combined data\n",
        "cooling_data_with_interactions_42 = pd.read_csv('cooling_data_with_interactions_42.csv')\n",
        "\n",
        "# Initialize KFold with 5 splits\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create storage for training and test data splits\n",
        "folds = []\n",
        "\n",
        "# Split the data into 5 folds\n",
        "for train_index, test_index in kf.split(cooling_data_with_interactions_42):\n",
        "    train_data = cooling_data_with_interactions_42.iloc[train_index]\n",
        "    test_data = cooling_data_with_interactions_42.iloc[test_index]\n",
        "    folds.append((train_data, test_data))\n",
        "\n",
        "# Save each training and test group as separate files\n",
        "for i, (train_data, test_data) in enumerate(folds):\n",
        "    train_data.to_csv(f'cooling_train_42_fold{i+1}.csv', index=False)\n",
        "    test_data.to_csv(f'cooling_test_42_fold{i+1}.csv', index=False)\n",
        "\n",
        "print(\"Training and test data for 5 folds with seed 42 have been generated and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVASFn-_Na2F"
      },
      "source": [
        "do the feature selection solely based on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHykW-loNgCJ",
        "outputId": "fb400e25-97f8-4873-b2d2-7e074bc972ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 features for cooling fold 1 with seed 42 saved.\n",
            "Top 25 features for cooling fold 2 with seed 42 saved.\n",
            "Top 25 features for cooling fold 3 with seed 42 saved.\n",
            "Top 25 features for cooling fold 4 with seed 42 saved.\n",
            "Top 25 features for cooling fold 5 with seed 42 saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Function to perform feature selection\n",
        "def feature_selection(train_data, target_column, preselected_features, k=25):\n",
        "    # Separate features and target\n",
        "    X = train_data.drop(columns=[target_column, 'TIMESTAMP'], errors='ignore')  # Drop TIMESTAMP\n",
        "    y = train_data[target_column]\n",
        "\n",
        "    # Train a RandomForestRegressor model\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=100)\n",
        "    rf_model.fit(X, y)\n",
        "\n",
        "    # Get feature names and their importance scores\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Combine feature names and scores into a DataFrame\n",
        "    selected_features = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Select the preselected features (label, T_out, RH_out)\n",
        "    preselected_df = selected_features[selected_features['Feature'].isin(preselected_features)]\n",
        "\n",
        "    # Select the remaining top features after excluding preselected ones\n",
        "    remaining_features = selected_features[~selected_features['Feature'].isin(preselected_features)]\n",
        "    top_remaining_features = remaining_features.head(k - len(preselected_features))\n",
        "\n",
        "    # Combine preselected features with the top remaining features\n",
        "    final_features = pd.concat([preselected_df, top_remaining_features])\n",
        "\n",
        "    return final_features\n",
        "\n",
        "# Perform feature selection for each fold\n",
        "target_column = 'WH_RTU_Total'\n",
        "preselected_features = ['label', 'T_out', 'RH_out']\n",
        "\n",
        "for i in range(1, 6):\n",
        "    train_data = pd.read_csv(f'cooling_train_42_fold{i}.csv')\n",
        "\n",
        "    # Perform feature selection\n",
        "    top_features = feature_selection(train_data, target_column, preselected_features, k=25)\n",
        "\n",
        "    # Save top features\n",
        "    top_features.to_csv(f'cooling_train_42_fold{i}_top25_features.csv', index=False)\n",
        "\n",
        "    print(f\"Top 25 features for cooling fold {i} with seed 42 saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YlDMNe2QA1D"
      },
      "source": [
        "prepare the cooling dataset's training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4b3nYkXQGfy",
        "outputId": "e74e1d28-2fb4-4d49-89ab-011b09b1b05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered cooling train data for fold 1 saved.\n",
            "Filtered cooling test data for fold 1 saved.\n",
            "Filtered cooling train data for fold 2 saved.\n",
            "Filtered cooling test data for fold 2 saved.\n",
            "Filtered cooling train data for fold 3 saved.\n",
            "Filtered cooling test data for fold 3 saved.\n",
            "Filtered cooling train data for fold 4 saved.\n",
            "Filtered cooling test data for fold 4 saved.\n",
            "Filtered cooling train data for fold 5 saved.\n",
            "Filtered cooling test data for fold 5 saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Target variable\n",
        "target_column = 'WH_RTU_Total'\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load the original training and test datasets for the current fold\n",
        "    train_data = pd.read_csv(f'cooling_train_42_fold{i}.csv')\n",
        "    test_data = pd.read_csv(f'cooling_test_42_fold{i}.csv')\n",
        "\n",
        "    # Load the top 25 features selected from the current fold\n",
        "    top_features = pd.read_csv(f'cooling_train_42_fold{i}_top25_features.csv')['Feature'].tolist()\n",
        "\n",
        "    # Ensure the target column is included in the selected features\n",
        "    filtered_columns = top_features + [target_column]  # Train data does not include 'label'\n",
        "\n",
        "    # Filter the training data\n",
        "    filtered_train_data = train_data[filtered_columns]\n",
        "    filtered_train_data.to_csv(f'fold{i}_cooling_filtered_train_42_data.csv', index=False)\n",
        "    print(f\"Filtered cooling train data for fold {i} saved.\")\n",
        "\n",
        "    # Filter the test data (temporarily including 'label')\n",
        "    filtered_test_data = test_data[filtered_columns]\n",
        "\n",
        "    # Save all datasets\n",
        "    filtered_test_data.to_csv(f'fold{i}_cooling_filtered_test_42_data.csv', index=False)\n",
        "    print(f\"Filtered cooling test data for fold {i} saved.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AhCXvSTRyvS"
      },
      "source": [
        "generate the mae and R2 value for the four models:linear regression, random forest ,xgboost and stacking ensemble regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXEa8mLQR7ka",
        "outputId": "ae8a134c-f4f9-41b7-d87d-743a3a97b246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n",
            "Fold 1, Model Linear Regression, MAE: 5.1169, R²: 0.4179\n",
            "Fold 1, Model Random Forest, MAE: 2.7831, R²: 0.6805\n",
            "Fold 1, Model XGBoost, MAE: 2.6708, R²: 0.7021\n",
            "Fold 1, Model Stacking Regressor, MAE: 2.6855, R²: 0.7085\n",
            "Fold 2, Model Linear Regression, MAE: 4.6243, R²: 0.5093\n",
            "Fold 2, Model Random Forest, MAE: 2.1688, R²: 0.7803\n",
            "Fold 2, Model XGBoost, MAE: 2.2281, R²: 0.7741\n",
            "Fold 2, Model Stacking Regressor, MAE: 2.1163, R²: 0.7900\n",
            "Fold 3, Model Linear Regression, MAE: 4.7232, R²: 0.5346\n",
            "Fold 3, Model Random Forest, MAE: 2.1348, R²: 0.8201\n",
            "Fold 3, Model XGBoost, MAE: 2.1085, R²: 0.8335\n",
            "Fold 3, Model Stacking Regressor, MAE: 2.0778, R²: 0.8331\n",
            "Fold 4, Model Linear Regression, MAE: 4.7064, R²: 0.4764\n",
            "Fold 4, Model Random Forest, MAE: 2.2271, R²: 0.7792\n",
            "Fold 4, Model XGBoost, MAE: 2.3552, R²: 0.7620\n",
            "Fold 4, Model Stacking Regressor, MAE: 2.2399, R²: 0.7819\n",
            "Fold 5, Model Linear Regression, MAE: 4.7443, R²: 0.4749\n",
            "Fold 5, Model Random Forest, MAE: 2.1909, R²: 0.8116\n",
            "Fold 5, Model XGBoost, MAE: 2.3158, R²: 0.7918\n",
            "Fold 5, Model Stacking Regressor, MAE: 2.1523, R²: 0.8120\n",
            "\n",
            "Summary Results for All Models (Cooling Data):\n",
            "                         MAE        R²\n",
            "Model                                 \n",
            "Stacking Regressor  2.254356  0.785106\n",
            "Random Forest       2.300940  0.774352\n",
            "XGBoost             2.335683  0.772673\n",
            "Linear Regression   4.783012  0.482639\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost\n",
        "!pip install scikit-learn==1.0.2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "# Placeholder for storing results\n",
        "results = []\n",
        "\n",
        "# Loop through each fold\n",
        "for i in range(1, 6):\n",
        "    # Load filtered training and test datasets for cooling data\n",
        "    train_data = pd.read_csv(f'fold{i}_cooling_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_cooling_filtered_test_42_data.csv')\n",
        "\n",
        "    # Separate features and target for training and testing data\n",
        "    target_column = 'WH_RTU_Total'\n",
        "    X_train = train_data.drop(columns=[target_column])\n",
        "    y_train = train_data[target_column]\n",
        "    X_test = test_data.drop(columns=[target_column])\n",
        "    y_test = test_data[target_column]\n",
        "\n",
        "    # Define models with default parameters\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(random_state=100),\n",
        "        'XGBoost': XGBRegressor(random_state=100),\n",
        "        'Stacking Regressor': StackingRegressor(\n",
        "            estimators=[\n",
        "                ('lr', LinearRegression()),\n",
        "                ('rf', RandomForestRegressor(random_state=100)),\n",
        "                ('xgb', XGBRegressor(random_state=100))\n",
        "            ]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_name, model in models.items():\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate MAE and R²\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'Fold': i,\n",
        "            'Seed': 42,\n",
        "            'Model': model_name,\n",
        "            'Dataset': 'Cooling',\n",
        "            'MAE': mae,\n",
        "            'R²': r2,\n",
        "            'Experiment':2\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {i}, Model {model_name}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('cooling_data_results_mae_r2.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for All Models (Cooling Data):\")\n",
        "summary = results_df.groupby('Model').agg({'MAE': 'mean', 'R²': 'mean'}).sort_values(by='MAE')\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoMivWWAfuFJ"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB5bSKiemn92"
      },
      "source": [
        "### hyper parameter tuning for cooling random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viqEAGG6mu0T",
        "outputId": "8084ea9f-4f76-4787-cc4e-0d274e44eec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Model Random Forest, Dataset Cooling, MAE: 2.7388, R²: 0.6882, Best Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 2, Model Random Forest, Dataset Cooling, MAE: 2.1836, R²: 0.7787, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 3, Model Random Forest, Dataset Cooling, MAE: 2.1184, R²: 0.8191, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 4, Model Random Forest, Dataset Cooling, MAE: 2.2043, R²: 0.7800, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Fold 5, Model Random Forest, Dataset Cooling, MAE: 2.1781, R²: 0.8090, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "\n",
            "Summary Results for Random Forest:\n",
            "           Model  Dataset       MAE        R²\n",
            "0  Random Forest  Cooling  2.284662  0.775014\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_rf = []\n",
        "\n",
        "# Define hyperparameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold{i}_cooling_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_cooling_filtered_test_42_data.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Initialize Random Forest and perform GridSearchCV\n",
        "    rf = RandomForestRegressor(random_state=100)\n",
        "    grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_rf = grid_search_rf.best_estimator_\n",
        "    best_params = grid_search_rf.best_params_\n",
        "\n",
        "    # Split test features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results_rf.append({\n",
        "        'Fold': i,\n",
        "        'Seed':42,\n",
        "        'Model': 'Random Forest',\n",
        "        'Dataset': 'Cooling',\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'Best Params': best_params,\n",
        "        'Experiment': 2\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {i}, Model Random Forest, Dataset Cooling, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_rf_df = pd.DataFrame(results_rf)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_rf_df.to_csv('random_forest_42_cooling_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for Random Forest:\")\n",
        "summary_rf = results_rf_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_rf)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_rf.to_csv('random_forest_42_cooling_summary_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBh34A3riemE"
      },
      "source": [
        "### Feature Importance for Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNnfoqF_jz0B",
        "outputId": "8a72d155-366b-44a5-9dfd-a0a600ca6ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 and experiment 2 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "# Fixed seed value\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_cooling_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    results_rf_df = pd.read_csv('random_forest_42_cooling_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_rf_df.loc[(results_rf_df['Fold'] == fold) & (results_rf_df['Dataset'] == 'Cooling'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train Random Forest model with best parameters\n",
        "    best_rf = RandomForestRegressor(random_state=100, **best_params_dict)\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importances = best_rf.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances,\n",
        "        'Fold': fold,\n",
        "        'Seed': 42,  # Add seed column\n",
        "        'Model': 'Random Forest',\n",
        "        'Experiment': experiment  # Add experiment column\n",
        "    })\n",
        "    feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine and save feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "feature_importance_combined.to_csv(f'random_forest_42_cooling_feature_importance_experiment_{experiment}.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 and experiment {experiment} has been successfully saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjlP0Zw9j87Y"
      },
      "source": [
        "### residual，predict，label，T_OUT and RH_OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0-tUjg6kDpz",
        "outputId": "42017eb7-c696-4757-d321-42bf6a0b1938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1.\n",
            "Processed fold 2.\n",
            "Processed fold 3.\n",
            "Processed fold 4.\n",
            "Processed fold 5.\n",
            "All folds processed and merged data saved successfully to random_forest_42_cooling_residual.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_cooling_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    best_params_df = pd.read_csv('random_forest_42_cooling_hyperparameter_results_detailed.csv')\n",
        "    best_params = best_params_df.loc[(best_params_df['Fold'] == fold) & (best_params_df['Dataset'] == 'Cooling'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train Random Forest model with best parameters\n",
        "    best_rf = RandomForestRegressor(random_state=100, **best_params_dict)\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(f'fold{fold}_cooling_filtered_test_42_data.csv')\n",
        "    test_data['Label'] = 'Cooling'  # Dynamically add label based on dataset type\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total', 'Label'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate residuals\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    test_data['Predicted'] = y_pred\n",
        "    test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "    # Select and rename required columns\n",
        "    if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "        selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'Label']].rename(columns={\n",
        "            'WH_RTU_Total': 'Actual'\n",
        "        })\n",
        "    else:\n",
        "        missing_columns = [col for col in ['T_out', 'RH_out'] if col not in test_data.columns]\n",
        "        raise ValueError(f\"{', '.join(missing_columns)} is/are missing in test data for fold {fold}\")\n",
        "\n",
        "    # Add fold, seed, and model columns\n",
        "    selected_data['Fold'] = fold\n",
        "    selected_data['Seed'] = 42\n",
        "    selected_data['Model'] = 'Random Forest'\n",
        "    selected_data['Experiment'] = experiment\n",
        "\n",
        "    # Append to final merged data\n",
        "    merged_data.append(selected_data)\n",
        "    print(f\"Processed fold {fold}.\")\n",
        "\n",
        "# Combine all folds into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "\n",
        "# Save the merged data\n",
        "output_file = 'random_forest_42_cooling_residual.csv'\n",
        "merged_final_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"All folds processed and merged data saved successfully to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZbHnBaumOUk"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcAlxKYo8QC"
      },
      "source": [
        "### hyper parameter tuning for cooling XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAasa5aupBWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab65535-2665-433d-ee0d-1c16e8f81166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n",
            "Fold 1, Model XGBoost, Dataset Cooling, MAE: 2.5667, R²: 0.7111, Best Params: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 2, Model XGBoost, Dataset Cooling, MAE: 2.1399, R²: 0.7921, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 3, Model XGBoost, Dataset Cooling, MAE: 1.9866, R²: 0.8425, Best Params: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 4, Model XGBoost, Dataset Cooling, MAE: 2.1882, R²: 0.7896, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 5, Model XGBoost, Dataset Cooling, MAE: 2.0958, R²: 0.8193, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "\n",
            "Summary Results for XGBoost:\n",
            "     Model  Dataset       MAE      R²\n",
            "0  XGBoost  Cooling  2.195429  0.7909\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost\n",
        "!pip install scikit-learn==1.0.2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_xgb = []\n",
        "experiment = 2  # Experiment number\n",
        "# Define hyperparameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold{i}_cooling_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_cooling_filtered_test_42_data.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Initialize XGBoost and perform GridSearchCV\n",
        "    xgb = XGBRegressor(random_state=100)\n",
        "    grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_xgb = grid_search_xgb.best_estimator_\n",
        "    best_params = grid_search_xgb.best_params_\n",
        "\n",
        "    # Split test features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = best_xgb.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results_xgb.append({\n",
        "        'Fold': i,\n",
        "        'Seed': 42,\n",
        "        'Model': 'XGBoost',\n",
        "        'Dataset': 'Cooling',\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'Best Params': best_params,\n",
        "        'Experiment' : experiment\n",
        "    })\n",
        "\n",
        "\n",
        "    print(f\"Fold {i}, Model XGBoost, Dataset Cooling, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_xgb_df = pd.DataFrame(results_xgb)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_xgb_df.to_csv('xgboost_42_cooling_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for XGBoost:\")\n",
        "summary_xgb = results_xgb_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_xgb)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_xgb.to_csv('xgboost_42_cooling_summary_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtaO99LPmcDD"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blKCszcwmig1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819a2caf-982c-4ca1-bfa7-9a41aa9234e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 and experiment 2 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_cooling_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Extract the best parameters from the results file\n",
        "    results_xgb_df = pd.read_csv('xgboost_42_cooling_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_xgb_df.loc[\n",
        "        (results_xgb_df['Fold'] == fold) & (results_xgb_df['Dataset'] == 'Cooling'),\n",
        "        'Best Params'\n",
        "    ].iloc[0]\n",
        "    best_params_dict = eval(best_params)  # Parse string into dictionary\n",
        "\n",
        "    # Initialize and train the model with the best parameters\n",
        "    best_xgb = XGBRegressor(random_state=100, **best_params_dict)\n",
        "    best_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importances = best_xgb.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances,\n",
        "        'Fold': fold,\n",
        "        'Seed': 42,  # Add seed column\n",
        "        'Model': 'XGBoost',\n",
        "        'Experiment': experiment  # Add experiment column\n",
        "    })\n",
        "    feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine all feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "\n",
        "# Save the feature importance results to a file\n",
        "feature_importance_combined.to_csv(f'xgboost_42_cooling_feature_importance_experiment_{experiment}.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 and experiment {experiment} has been successfully saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuaD1tI_mi9i"
      },
      "source": [
        "### Residual analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noocNEDYmoB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56270d1-dc9d-4f9c-f44f-ea938c852b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1.\n",
            "Processed fold 2.\n",
            "Processed fold 3.\n",
            "Processed fold 4.\n",
            "Processed fold 5.\n",
            "All folds processed and merged data saved successfully to xgboost_42_cooling_residual.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_cooling_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    best_params_df = pd.read_csv('xgboost_42_cooling_hyperparameter_results_detailed.csv')\n",
        "    best_params = best_params_df.loc[(best_params_df['Fold'] == fold) & (best_params_df['Dataset'] == 'Cooling'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train XGBoost model with best parameters\n",
        "    best_xgb = XGBRegressor(random_state=100, **best_params_dict)\n",
        "    best_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(f'fold{fold}_cooling_filtered_test_42_data.csv')\n",
        "    test_data['Label'] = 'Cooling'  # Dynamically add label based on dataset type\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total', 'Label'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate residuals\n",
        "    y_pred = best_xgb.predict(X_test)\n",
        "    test_data['Predicted'] = y_pred\n",
        "    test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "    # Select and rename required columns\n",
        "    if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "        selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'Label']].rename(columns={\n",
        "            'WH_RTU_Total': 'Actual'\n",
        "        })\n",
        "    else:\n",
        "        missing_columns = [col for col in ['T_out', 'RH_out'] if col not in test_data.columns]\n",
        "        raise ValueError(f\"{', '.join(missing_columns)} is/are missing in test data for fold {fold}\")\n",
        "\n",
        "    # Add fold, seed, and model columns\n",
        "    selected_data['Fold'] = fold\n",
        "    selected_data['Seed'] = 42\n",
        "    selected_data['Model'] = 'XGBoost'\n",
        "    selected_data['Experiment'] = experiment\n",
        "\n",
        "    # Append to final merged data\n",
        "    merged_data.append(selected_data)\n",
        "    print(f\"Processed fold {fold}.\")\n",
        "\n",
        "# Combine all folds into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "\n",
        "# Save the merged data\n",
        "output_file = 'xgboost_42_cooling_residual.csv'\n",
        "merged_final_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"All folds processed and merged data saved successfully to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsAc0-M4oAwU"
      },
      "source": [
        "## Stacking Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaPrtDkupmxG"
      },
      "source": [
        "### hyper parameter tuning for cooling Stacking Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OFl-su9pnko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2e5694-2314-4cbf-c290-266485c3fbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Model Stacking Regressor, Dataset Cooling, MAE: 2.6437, R²: 0.7122, Best Params: {'final_estimator__fit_intercept': True, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 50}\n",
            "Fold 2, Model Stacking Regressor, Dataset Cooling, MAE: 2.1481, R²: 0.7853, Best Params: {'final_estimator__fit_intercept': True, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
            "Fold 3, Model Stacking Regressor, Dataset Cooling, MAE: 2.0945, R²: 0.8251, Best Params: {'final_estimator__fit_intercept': True, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
            "Fold 4, Model Stacking Regressor, Dataset Cooling, MAE: 2.1366, R²: 0.7957, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 5, Model Stacking Regressor, Dataset Cooling, MAE: 2.1276, R²: 0.8115, Best Params: {'final_estimator__fit_intercept': True, 'gb__learning_rate': 0.01, 'gb__max_depth': 3, 'rf__max_depth': 10, 'rf__n_estimators': 100}\n",
            "\n",
            "Summary Results for Stacking Regressor:\n",
            "                Model  Dataset       MAE        R²\n",
            "0  Stacking Regressor  Cooling  2.230108  0.785968\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_stacking = []\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "# Define hyperparameter grid for Stacking Regressor\n",
        "param_grid_stacking = {\n",
        "    'final_estimator__fit_intercept': [True, False],  # Meta-model parameter (LinearRegression)\n",
        "    'rf__n_estimators': [50, 100],                   # Base model: Random Forest\n",
        "    'rf__max_depth': [10, 20],                       # Base model: Random Forest\n",
        "    'gb__learning_rate': [0.01, 0.1],                # Base model: Gradient Boosting\n",
        "    'gb__max_depth': [3, 6]                          # Base model: Gradient Boosting\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold{i}_cooling_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_cooling_filtered_test_42_data.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Define base models and Stacking Regressor\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100)),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100))\n",
        "    ]\n",
        "    meta_model = LinearRegression()\n",
        "    stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "    # Perform GridSearchCV on the Stacking Regressor\n",
        "    grid_search_stacking = GridSearchCV(stacking, param_grid_stacking, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_stacking = grid_search_stacking.best_estimator_\n",
        "    best_params = grid_search_stacking.best_params_\n",
        "\n",
        "    # Split test features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = best_stacking.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results_stacking.append({\n",
        "        'Fold': i,\n",
        "        'Seed': 42,\n",
        "        'Model': 'Stacking Regressor',\n",
        "        'Dataset': 'Cooling',\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'Best Params': best_params,\n",
        "        'Experiment': experiment\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {i}, Model Stacking Regressor, Dataset Cooling, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_stacking_df = pd.DataFrame(results_stacking)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_stacking_df.to_csv('stacking_42_cooling_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for Stacking Regressor:\")\n",
        "summary_stacking = results_stacking_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_stacking)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_stacking.to_csv('stacking_42_cooling_summary_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka0OE08_oJYW"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xbNcEqToMGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f366895-c692-42b7-96ef-7bd5ed6eb576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 and experiment 2 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_cooling_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Extract the best parameters from the results file\n",
        "    results_stacking_df = pd.read_csv('stacking_42_cooling_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_stacking_df.loc[\n",
        "        (results_stacking_df['Fold'] == fold) & (results_stacking_df['Dataset'] == 'Cooling'),\n",
        "        'Best Params'\n",
        "    ].iloc[0]\n",
        "    best_params_dict = eval(best_params)  # Parse string into dictionary\n",
        "\n",
        "    # Define base models\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100, **{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('rf__')})),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100, **{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('gb__')}))\n",
        "    ]\n",
        "    meta_model = LinearRegression(**{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('final_estimator__')})\n",
        "\n",
        "    # Define Stacking Regressor and fit the model\n",
        "    stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "    stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importances from base models\n",
        "    for model_name, model in stacking.named_estimators_.items():\n",
        "        if hasattr(model, 'feature_importances_'):  # Check if the model supports feature importance\n",
        "            feature_importances = model.feature_importances_\n",
        "            feature_names = X_train.columns\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': feature_importances,\n",
        "                'Model': model_name,\n",
        "                'Fold': fold,\n",
        "                'Seed': 42,\n",
        "                'Experiment': experiment  # Add experiment column\n",
        "            })\n",
        "            feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine all feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "\n",
        "# Save the feature importance results to a file\n",
        "feature_importance_combined.to_csv(f'stacking_42_cooling_feature_importance_experiment_{experiment}.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 and experiment {experiment} has been successfully saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdlCtmVooMrS"
      },
      "source": [
        "### Residual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIzbglcNoPzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ede5c1-aa95-4735-94d6-cf3fa80ac346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1.\n",
            "Processed fold 2.\n",
            "Processed fold 3.\n",
            "Processed fold 4.\n",
            "Processed fold 5.\n",
            "All folds processed and merged data saved successfully to stacking_42_cooling_residual.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Load the best model parameters for each fold\n",
        "best_params_df = pd.read_csv('stacking_42_cooling_hyperparameter_results_detailed.csv')\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_cooling_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Get the best parameters for the current fold\n",
        "    best_params = best_params_df.loc[best_params_df['Fold'] == fold, 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Define base models and Stacking Regressor with best parameters\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100,\n",
        "                                     n_estimators=best_params_dict['rf__n_estimators'],\n",
        "                                     max_depth=best_params_dict['rf__max_depth'])),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100,\n",
        "                                         learning_rate=best_params_dict['gb__learning_rate'],\n",
        "                                         max_depth=best_params_dict['gb__max_depth']))\n",
        "    ]\n",
        "    meta_model = LinearRegression(fit_intercept=best_params_dict['final_estimator__fit_intercept'])\n",
        "    best_stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "    # Train the Stacking Regressor\n",
        "    best_stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(f'fold{fold}_cooling_filtered_test_42_data.csv')\n",
        "    test_data['Label'] = 'Cooling'  # Dynamically add label based on dataset type\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total', 'Label'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate residuals\n",
        "    y_pred = best_stacking.predict(X_test)\n",
        "    test_data = test_data.reset_index(drop=True)  # Reset test_data index\n",
        "    y_pred_series = pd.Series(y_pred, index=test_data.index)  # Align y_pred with test_data index\n",
        "    test_data['Predicted'] = y_pred_series\n",
        "    test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "    # Select and rename required columns\n",
        "    if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "        selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'Label']].rename(columns={\n",
        "            'WH_RTU_Total': 'Actual'\n",
        "        })\n",
        "    else:\n",
        "        missing_columns = [col for col in ['T_out', 'RH_out'] if col not in test_data.columns]\n",
        "        raise ValueError(f\"{', '.join(missing_columns)} is/are missing in test data for fold {fold}\")\n",
        "\n",
        "    # Add fold, seed, and model columns\n",
        "    selected_data['Fold'] = fold\n",
        "    selected_data['Seed'] = 42\n",
        "    selected_data['Model'] = 'StackingRegressor'\n",
        "    selected_data['Experiment'] = experiment\n",
        "\n",
        "    # Append to final merged data\n",
        "    merged_data.append(selected_data)\n",
        "    print(f\"Processed fold {fold}.\")\n",
        "\n",
        "# Combine all folds into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "\n",
        "# Save the merged data\n",
        "output_file = 'stacking_42_cooling_residual.csv'\n",
        "merged_final_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"All folds processed and merged data saved successfully to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4lVX_r2DINK"
      },
      "source": [
        "# heating Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7bggNUkDINL"
      },
      "source": [
        "do the same model performance for heating data, first divide the heating data into 5 folds and generate differet training and testing groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c3b5c9-fa37-43b5-a3ee-01da4006584d",
        "id": "gJm_mW16DINL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and test data for 5 folds with seed 42 have been generated and saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load the combined data\n",
        "heating_data_with_interactions_42 = pd.read_csv('heating_data_with_interactions_42.csv')\n",
        "\n",
        "# Initialize KFold with 5 splits\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create storage for training and test data splits\n",
        "folds = []\n",
        "\n",
        "# Split the data into 5 folds\n",
        "for train_index, test_index in kf.split(heating_data_with_interactions_42):\n",
        "    train_data = heating_data_with_interactions_42.iloc[train_index]\n",
        "    test_data = heating_data_with_interactions_42.iloc[test_index]\n",
        "    folds.append((train_data, test_data))\n",
        "\n",
        "# Save each training and test group as separate files\n",
        "for i, (train_data, test_data) in enumerate(folds):\n",
        "    train_data.to_csv(f'heating_train_42_fold{i+1}.csv', index=False)\n",
        "    test_data.to_csv(f'heating_test_42_fold{i+1}.csv', index=False)\n",
        "\n",
        "print(\"Training and test data for 5 folds with seed 42 have been generated and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyzy892oDINM"
      },
      "source": [
        "do the feature selection solely based on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "e5e2ccad-5add-45c5-b8cb-835e14956c09",
        "id": "KSVwRzwkDINM"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d32a22e2e921>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Perform feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtop_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreselected_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Save top features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d32a22e2e921>\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(train_data, target_column, preselected_features, k)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train a RandomForestRegressor model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get feature names and their importance scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Function to perform feature selection\n",
        "def feature_selection(train_data, target_column, preselected_features, k=25):\n",
        "    # Separate features and target\n",
        "    X = train_data.drop(columns=[target_column, 'TIMESTAMP'], errors='ignore')  # Drop TIMESTAMP\n",
        "    y = train_data[target_column]\n",
        "\n",
        "    # Train a RandomForestRegressor model\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=100)\n",
        "    rf_model.fit(X, y)\n",
        "\n",
        "    # Get feature names and their importance scores\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Combine feature names and scores into a DataFrame\n",
        "    selected_features = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Select the preselected features (label, T_out, RH_out)\n",
        "    preselected_df = selected_features[selected_features['Feature'].isin(preselected_features)]\n",
        "\n",
        "    # Select the remaining top features after excluding preselected ones\n",
        "    remaining_features = selected_features[~selected_features['Feature'].isin(preselected_features)]\n",
        "    top_remaining_features = remaining_features.head(k - len(preselected_features))\n",
        "\n",
        "    # Combine preselected features with the top remaining features\n",
        "    final_features = pd.concat([preselected_df, top_remaining_features])\n",
        "\n",
        "    return final_features\n",
        "\n",
        "# Perform feature selection for each fold\n",
        "target_column = 'WH_RTU_Total'\n",
        "preselected_features = ['label', 'T_out', 'RH_out']\n",
        "\n",
        "for i in range(1, 6):\n",
        "    train_data = pd.read_csv(f'heating_train_42_fold{i}.csv')\n",
        "\n",
        "    # Perform feature selection\n",
        "    top_features = feature_selection(train_data, target_column, preselected_features, k=25)\n",
        "\n",
        "    # Save top features\n",
        "    top_features.to_csv(f'heating_train_42_fold{i}_top25_features.csv', index=False)\n",
        "\n",
        "    print(f\"Top 25 features for heating fold {i} with seed 42 saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAZBPdLUDINM"
      },
      "source": [
        "prepare the heating dataset's training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d75a28e-ae3d-479b-c3a8-87223a0e6118",
        "id": "QZkURa4dDINN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered heating train data for fold 1 saved.\n",
            "Filtered heating test data for fold 1 saved.\n",
            "Filtered heating train data for fold 2 saved.\n",
            "Filtered heating test data for fold 2 saved.\n",
            "Filtered heating train data for fold 3 saved.\n",
            "Filtered heating test data for fold 3 saved.\n",
            "Filtered heating train data for fold 4 saved.\n",
            "Filtered heating test data for fold 4 saved.\n",
            "Filtered heating train data for fold 5 saved.\n",
            "Filtered heating test data for fold 5 saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Target variable\n",
        "target_column = 'WH_RTU_Total'\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load the original training and test datasets for the current fold\n",
        "    train_data = pd.read_csv(f'heating_train_42_fold{i}.csv')\n",
        "    test_data = pd.read_csv(f'heating_test_42_fold{i}.csv')\n",
        "\n",
        "    # Load the top 25 features selected from the current fold\n",
        "    top_features = pd.read_csv(f'heating_train_42_fold{i}_top25_features.csv')['Feature'].tolist()\n",
        "\n",
        "    # Ensure the target column is included in the selected features\n",
        "    filtered_columns = top_features + [target_column]  # Train data does not include 'label'\n",
        "\n",
        "    # Filter the training data\n",
        "    filtered_train_data = train_data[filtered_columns]\n",
        "    filtered_train_data.to_csv(f'fold{i}_heating_filtered_train_42_data.csv', index=False)\n",
        "    print(f\"Filtered heating train data for fold {i} saved.\")\n",
        "\n",
        "    # Filter the test data (temporarily including 'label')\n",
        "    filtered_test_data = test_data[filtered_columns]\n",
        "\n",
        "    # Save all datasets\n",
        "    filtered_test_data.to_csv(f'fold{i}_heating_filtered_test_42_data.csv', index=False)\n",
        "    print(f\"Filtered heating test data for fold {i} saved.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IySI1Vf-DINN"
      },
      "source": [
        "generate the mae and R2 value for the four models:linear regression, random forest ,xgboost and stacking ensemble regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb024569-a6d1-4afb-8bc5-0e04d9b826c7",
        "id": "8PSp7cFIDINN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n",
            "Fold 1, Model Linear Regression, MAE: 3.5158, R²: 0.3482\n",
            "Fold 1, Model Random Forest, MAE: 2.8247, R²: 0.5646\n",
            "Fold 1, Model XGBoost, MAE: 2.8222, R²: 0.5362\n",
            "Fold 1, Model Stacking Regressor, MAE: 2.9881, R²: 0.5380\n",
            "Fold 2, Model Linear Regression, MAE: 3.5004, R²: 0.3841\n",
            "Fold 2, Model Random Forest, MAE: 2.9498, R²: 0.5709\n",
            "Fold 2, Model XGBoost, MAE: 2.7744, R²: 0.5668\n",
            "Fold 2, Model Stacking Regressor, MAE: 3.0451, R²: 0.5506\n",
            "Fold 3, Model Linear Regression, MAE: 3.7288, R²: 0.3719\n",
            "Fold 3, Model Random Forest, MAE: 3.2049, R²: 0.5496\n",
            "Fold 3, Model XGBoost, MAE: 3.2185, R²: 0.5081\n",
            "Fold 3, Model Stacking Regressor, MAE: 3.2651, R²: 0.5450\n",
            "Fold 4, Model Linear Regression, MAE: 3.6318, R²: 0.4350\n",
            "Fold 4, Model Random Forest, MAE: 2.9965, R²: 0.6144\n",
            "Fold 4, Model XGBoost, MAE: 2.9355, R²: 0.6128\n",
            "Fold 4, Model Stacking Regressor, MAE: 3.0255, R²: 0.6268\n",
            "Fold 5, Model Linear Regression, MAE: 3.5106, R²: 0.4494\n",
            "Fold 5, Model Random Forest, MAE: 2.8340, R²: 0.6148\n",
            "Fold 5, Model XGBoost, MAE: 2.8057, R²: 0.5804\n",
            "Fold 5, Model Stacking Regressor, MAE: 2.9785, R²: 0.5961\n",
            "\n",
            "Summary Results for All Models (heating Data):\n",
            "                         MAE        R²\n",
            "Model                                 \n",
            "XGBoost             2.911263  0.560857\n",
            "Random Forest       2.961997  0.582855\n",
            "Stacking Regressor  3.060462  0.571285\n",
            "Linear Regression   3.577486  0.397704\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost\n",
        "!pip install scikit-learn==1.0.2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "# Placeholder for storing results\n",
        "results = []\n",
        "\n",
        "# Loop through each fold\n",
        "for i in range(1, 6):\n",
        "    # Load filtered training and test datasets for heating data\n",
        "    train_data = pd.read_csv(f'fold{i}_heating_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_heating_filtered_test_42_data.csv')\n",
        "\n",
        "    # Separate features and target for training and testing data\n",
        "    target_column = 'WH_RTU_Total'\n",
        "    X_train = train_data.drop(columns=[target_column])\n",
        "    y_train = train_data[target_column]\n",
        "    X_test = test_data.drop(columns=[target_column])\n",
        "    y_test = test_data[target_column]\n",
        "\n",
        "    # Define models with default parameters\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(random_state=100),\n",
        "        'XGBoost': XGBRegressor(random_state=100),\n",
        "        'Stacking Regressor': StackingRegressor(\n",
        "            estimators=[\n",
        "                ('lr', LinearRegression()),\n",
        "                ('rf', RandomForestRegressor(random_state=100)),\n",
        "                ('xgb', XGBRegressor(random_state=100))\n",
        "            ]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_name, model in models.items():\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate MAE and R²\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'Fold': i,\n",
        "            'Seed': 42,\n",
        "            'Model': model_name,\n",
        "            'Dataset': 'heating',\n",
        "            'MAE': mae,\n",
        "            'R²': r2,\n",
        "            'Experiment':2\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {i}, Model {model_name}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('heating_data_results_mae_r2.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for All Models (heating Data):\")\n",
        "summary = results_df.groupby('Model').agg({'MAE': 'mean', 'R²': 'mean'}).sort_values(by='MAE')\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy6WhflKDINO"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XPjaG54DINO"
      },
      "source": [
        "### hyper parameter tuning for heating random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e084329-8551-4965-ad23-1faf4bbda9fa",
        "id": "GFCxCmRdDINO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Model Random Forest, Dataset heating, MAE: 2.8347, R²: 0.5615, Best Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 2, Model Random Forest, Dataset heating, MAE: 2.9154, R²: 0.5796, Best Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 3, Model Random Forest, Dataset heating, MAE: 3.2533, R²: 0.5425, Best Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Fold 4, Model Random Forest, Dataset heating, MAE: 3.0317, R²: 0.6062, Best Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Fold 5, Model Random Forest, Dataset heating, MAE: 2.8340, R²: 0.6148, Best Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Summary Results for Random Forest:\n",
            "           Model  Dataset       MAE       R²\n",
            "0  Random Forest  heating  2.973816  0.58092\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_rf = []\n",
        "\n",
        "# Define hyperparameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold{i}_heating_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_heating_filtered_test_42_data.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Initialize Random Forest and perform GridSearchCV\n",
        "    rf = RandomForestRegressor(random_state=100)\n",
        "    grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_rf = grid_search_rf.best_estimator_\n",
        "    best_params = grid_search_rf.best_params_\n",
        "\n",
        "    # Split test features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results_rf.append({\n",
        "        'Fold': i,\n",
        "        'Seed':42,\n",
        "        'Model': 'Random Forest',\n",
        "        'Dataset': 'heating',\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'Best Params': best_params,\n",
        "        'Experiment': 2\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {i}, Model Random Forest, Dataset heating, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_rf_df = pd.DataFrame(results_rf)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_rf_df.to_csv('random_forest_42_heating_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for Random Forest:\")\n",
        "summary_rf = results_rf_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_rf)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_rf.to_csv('random_forest_42_heating_summary_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTliZY-SDINO"
      },
      "source": [
        "### Feature Importance for Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254eee09-8ab3-46dc-9c84-eeab888d0a0f",
        "id": "ERasBc1IDINP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 and experiment 2 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "# Fixed seed value\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_heating_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    results_rf_df = pd.read_csv('random_forest_42_heating_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_rf_df.loc[(results_rf_df['Fold'] == fold) & (results_rf_df['Dataset'] == 'heating'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train Random Forest model with best parameters\n",
        "    best_rf = RandomForestRegressor(random_state=100, **best_params_dict)\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importances = best_rf.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances,\n",
        "        'Fold': fold,\n",
        "        'Seed': 42,  # Add seed column\n",
        "        'Model': 'Random Forest',\n",
        "        'Experiment': experiment  # Add experiment column\n",
        "    })\n",
        "    feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine and save feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "feature_importance_combined.to_csv(f'random_forest_42_heating_feature_importance_experiment_{experiment}.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 and experiment {experiment} has been successfully saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pVfvqX8DINP"
      },
      "source": [
        "### residual，predict，label，T_OUT and RH_OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd665a4f-e1a4-41df-de8e-68f07722d0f7",
        "id": "WiVDDnlWDINP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1.\n",
            "Processed fold 2.\n",
            "Processed fold 3.\n",
            "Processed fold 4.\n",
            "Processed fold 5.\n",
            "All folds processed and merged data saved successfully to random_forest_42_heating_residual.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Fixed seed value\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_heating_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    best_params_df = pd.read_csv('random_forest_42_heating_hyperparameter_results_detailed.csv')\n",
        "    best_params = best_params_df.loc[(best_params_df['Fold'] == fold) & (best_params_df['Dataset'] == 'heating'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train Random Forest model with best parameters\n",
        "    best_rf = RandomForestRegressor(random_state=100, **best_params_dict)\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(f'fold{fold}_heating_filtered_test_42_data.csv')\n",
        "    test_data['Label'] = 'heating'  # Dynamically add label based on dataset type\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total', 'Label'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate residuals\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    test_data['Predicted'] = y_pred\n",
        "    test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "    # Select and rename required columns\n",
        "    if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "        selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'Label']].rename(columns={\n",
        "            'WH_RTU_Total': 'Actual'\n",
        "        })\n",
        "    else:\n",
        "        missing_columns = [col for col in ['T_out', 'RH_out'] if col not in test_data.columns]\n",
        "        raise ValueError(f\"{', '.join(missing_columns)} is/are missing in test data for fold {fold}\")\n",
        "\n",
        "    # Add fold, seed, and model columns\n",
        "    selected_data['Fold'] = fold\n",
        "    selected_data['Seed'] = 42\n",
        "    selected_data['Model'] = 'Random Forest'\n",
        "    selected_data['Experiment'] = experiment\n",
        "\n",
        "    # Append to final merged data\n",
        "    merged_data.append(selected_data)\n",
        "    print(f\"Processed fold {fold}.\")\n",
        "\n",
        "# Combine all folds into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "\n",
        "# Save the merged data\n",
        "output_file = 'random_forest_42_heating_residual.csv'\n",
        "merged_final_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"All folds processed and merged data saved successfully to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOcVzSHRDINR"
      },
      "source": [
        "## XGBOOST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBJXELeDINR"
      },
      "source": [
        "### hyper parameter tuning for heating XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f84082-dbfc-41d9-8ab5-4c53f2c2289f",
        "id": "Om7L9LdsDINS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.5.0)\n",
            "Fold 1, Model XGBoost, Dataset heating, MAE: 2.7617, R²: 0.5551, Best Params: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 2, Model XGBoost, Dataset heating, MAE: 2.7254, R²: 0.6055, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 3, Model XGBoost, Dataset heating, MAE: 3.2929, R²: 0.5191, Best Params: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Fold 4, Model XGBoost, Dataset heating, MAE: 2.9266, R²: 0.6234, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8}\n",
            "Fold 5, Model XGBoost, Dataset heating, MAE: 2.7558, R²: 0.6267, Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Summary Results for XGBoost:\n",
            "     Model  Dataset       MAE        R²\n",
            "0  XGBoost  heating  2.892474  0.585945\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost\n",
        "!pip install scikit-learn==1.0.2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_xgb = []\n",
        "experiment = 2  # Experiment number\n",
        "# Define hyperparameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold{i}_heating_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_heating_filtered_test_42_data.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Initialize XGBoost and perform GridSearchCV\n",
        "    xgb = XGBRegressor(random_state=100)\n",
        "    grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_xgb = grid_search_xgb.best_estimator_\n",
        "    best_params = grid_search_xgb.best_params_\n",
        "\n",
        "    # Split test features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = best_xgb.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results_xgb.append({\n",
        "        'Fold': i,\n",
        "        'Seed': 42,\n",
        "        'Model': 'XGBoost',\n",
        "        'Dataset': 'heating',\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'Best Params': best_params,\n",
        "        'Experiment' : experiment\n",
        "    })\n",
        "\n",
        "\n",
        "    print(f\"Fold {i}, Model XGBoost, Dataset heating, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_xgb_df = pd.DataFrame(results_xgb)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_xgb_df.to_csv('xgboost_42_heating_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for XGBoost:\")\n",
        "summary_xgb = results_xgb_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_xgb)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_xgb.to_csv('xgboost_42_heating_summary_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w2EzdepDINS"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6bf2a3-78c2-441a-f9b1-87fe753653a3",
        "id": "lBTi3ADMDINT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 and experiment 2 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "# Fixed random seed\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_heating_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Extract the best parameters from the results file\n",
        "    results_xgb_df = pd.read_csv('xgboost_42_heating_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_xgb_df.loc[\n",
        "        (results_xgb_df['Fold'] == fold) & (results_xgb_df['Dataset'] == 'heating'),\n",
        "        'Best Params'\n",
        "    ].iloc[0]\n",
        "    best_params_dict = eval(best_params)  # Parse string into dictionary\n",
        "\n",
        "    # Initialize and train the model with the best parameters\n",
        "    best_xgb = XGBRegressor(random_state=100, **best_params_dict)\n",
        "    best_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importances = best_xgb.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importances,\n",
        "        'Fold': fold,\n",
        "        'Seed': 42,  # Add seed column\n",
        "        'Model': 'XGBoost',\n",
        "        'Experiment': experiment  # Add experiment column\n",
        "    })\n",
        "    feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine all feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "\n",
        "# Save the feature importance results to a file\n",
        "feature_importance_combined.to_csv(f'xgboost_42_heating_feature_importance_experiment_{experiment}.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 and experiment {experiment} has been successfully saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtk1rEMvDINT"
      },
      "source": [
        "### Residual analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbf26db-49fe-4fb7-ab7a-0dace2a0e142",
        "id": "SWkU7sHkDINT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1.\n",
            "Processed fold 2.\n",
            "Processed fold 3.\n",
            "Processed fold 4.\n",
            "Processed fold 5.\n",
            "All folds processed and merged data saved successfully to xgboost_42_heating_residual.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_heating_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Load the best model parameters for the current fold\n",
        "    best_params_df = pd.read_csv('xgboost_42_heating_hyperparameter_results_detailed.csv')\n",
        "    best_params = best_params_df.loc[(best_params_df['Fold'] == fold) & (best_params_df['Dataset'] == 'heating'), 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Train XGBoost model with best parameters\n",
        "    best_xgb = XGBRegressor(random_state=100, **best_params_dict)\n",
        "    best_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(f'fold{fold}_heating_filtered_test_42_data.csv')\n",
        "    test_data['Label'] = 'heating'  # Dynamically add label based on dataset type\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total', 'Label'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate residuals\n",
        "    y_pred = best_xgb.predict(X_test)\n",
        "    test_data['Predicted'] = y_pred\n",
        "    test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "    # Select and rename required columns\n",
        "    if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "        selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'Label']].rename(columns={\n",
        "            'WH_RTU_Total': 'Actual'\n",
        "        })\n",
        "    else:\n",
        "        missing_columns = [col for col in ['T_out', 'RH_out'] if col not in test_data.columns]\n",
        "        raise ValueError(f\"{', '.join(missing_columns)} is/are missing in test data for fold {fold}\")\n",
        "\n",
        "    # Add fold, seed, and model columns\n",
        "    selected_data['Fold'] = fold\n",
        "    selected_data['Seed'] = 42\n",
        "    selected_data['Model'] = 'XGBoost'\n",
        "    selected_data['Experiment'] = experiment\n",
        "\n",
        "    # Append to final merged data\n",
        "    merged_data.append(selected_data)\n",
        "    print(f\"Processed fold {fold}.\")\n",
        "\n",
        "# Combine all folds into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "\n",
        "# Save the merged data\n",
        "output_file = 'xgboost_42_heating_residual.csv'\n",
        "merged_final_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"All folds processed and merged data saved successfully to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ81VcOpDINT"
      },
      "source": [
        "## Stacking Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKcY0Z2wDINU"
      },
      "source": [
        "### hyper parameter tuning for heating Stacking Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944213ef-fb07-45c9-bbc0-8dcb67909d96",
        "id": "10TYJ-9dDINU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Model Stacking Regressor, Dataset heating, MAE: 2.7927, R²: 0.5717, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 2, Model Stacking Regressor, Dataset heating, MAE: 2.8901, R²: 0.5801, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 50}\n",
            "Fold 3, Model Stacking Regressor, Dataset heating, MAE: 3.2133, R²: 0.5485, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "Fold 4, Model Stacking Regressor, Dataset heating, MAE: 3.0033, R²: 0.6176, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 10, 'rf__n_estimators': 50}\n",
            "Fold 5, Model Stacking Regressor, Dataset heating, MAE: 2.8292, R²: 0.6159, Best Params: {'final_estimator__fit_intercept': False, 'gb__learning_rate': 0.1, 'gb__max_depth': 6, 'rf__max_depth': 20, 'rf__n_estimators': 100}\n",
            "\n",
            "Summary Results for Stacking Regressor:\n",
            "                Model  Dataset       MAE        R²\n",
            "0  Stacking Regressor  heating  2.945716  0.586763\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Placeholder for storing results\n",
        "results_stacking = []\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "# Define hyperparameter grid for Stacking Regressor\n",
        "param_grid_stacking = {\n",
        "    'final_estimator__fit_intercept': [True, False],  # Meta-model parameter (LinearRegression)\n",
        "    'rf__n_estimators': [50, 100],                   # Base model: Random Forest\n",
        "    'rf__max_depth': [10, 20],                       # Base model: Random Forest\n",
        "    'gb__learning_rate': [0.01, 0.1],                # Base model: Gradient Boosting\n",
        "    'gb__max_depth': [3, 6]                          # Base model: Gradient Boosting\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(f'fold{i}_heating_filtered_train_42_data.csv')\n",
        "    test_data = pd.read_csv(f'fold{i}_heating_filtered_test_42_data.csv')\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Define base models and Stacking Regressor\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100)),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100))\n",
        "    ]\n",
        "    meta_model = LinearRegression()\n",
        "    stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "    # Perform GridSearchCV on the Stacking Regressor\n",
        "    grid_search_stacking = GridSearchCV(stacking, param_grid_stacking, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    grid_search_stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model and hyperparameters\n",
        "    best_stacking = grid_search_stacking.best_estimator_\n",
        "    best_params = grid_search_stacking.best_params_\n",
        "\n",
        "    # Split test features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    y_pred = best_stacking.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results_stacking.append({\n",
        "        'Fold': i,\n",
        "        'Seed': 42,\n",
        "        'Model': 'Stacking Regressor',\n",
        "        'Dataset': 'heating',\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'Best Params': best_params,\n",
        "        'Experiment': experiment\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {i}, Model Stacking Regressor, Dataset heating, MAE: {mae:.4f}, R²: {r2:.4f}, Best Params: {best_params}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_stacking_df = pd.DataFrame(results_stacking)\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_stacking_df.to_csv('stacking_42_heating_hyperparameter_results_detailed.csv', index=False)\n",
        "\n",
        "# Display overall summary\n",
        "print(\"\\nSummary Results for Stacking Regressor:\")\n",
        "summary_stacking = results_stacking_df.groupby(['Model', 'Dataset']).agg({'MAE': 'mean', 'R²': 'mean'}).reset_index()\n",
        "print(summary_stacking)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_stacking.to_csv('stacking_42_heating_summary_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgHnpWSrDINU"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d5f156-546c-4bb5-bc77-7ac0d7dd6cf2",
        "id": "l0U__hbvDINU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance for seed 42 and experiment 2 has been successfully saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "\n",
        "# Placeholder for feature importance results\n",
        "feature_importance_results = []\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "\n",
        "# Section 1: Calculate Feature Importance\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_heating_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Extract the best parameters from the results file\n",
        "    results_stacking_df = pd.read_csv('stacking_42_heating_hyperparameter_results_detailed.csv')\n",
        "    best_params = results_stacking_df.loc[\n",
        "        (results_stacking_df['Fold'] == fold) & (results_stacking_df['Dataset'] == 'heating'),\n",
        "        'Best Params'\n",
        "    ].iloc[0]\n",
        "    best_params_dict = eval(best_params)  # Parse string into dictionary\n",
        "\n",
        "    # Define base models\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100, **{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('rf__')})),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100, **{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('gb__')}))\n",
        "    ]\n",
        "    meta_model = LinearRegression(**{k.split('__')[1]: v for k, v in best_params_dict.items() if k.startswith('final_estimator__')})\n",
        "\n",
        "    # Define Stacking Regressor and fit the model\n",
        "    stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "    stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate feature importances from base models\n",
        "    for model_name, model in stacking.named_estimators_.items():\n",
        "        if hasattr(model, 'feature_importances_'):  # Check if the model supports feature importance\n",
        "            feature_importances = model.feature_importances_\n",
        "            feature_names = X_train.columns\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': feature_importances,\n",
        "                'Model': model_name,\n",
        "                'Fold': fold,\n",
        "                'Seed': 42,\n",
        "                'Experiment': experiment  # Add experiment column\n",
        "            })\n",
        "            feature_importance_results.append(importance_df)\n",
        "\n",
        "# Combine all feature importance results\n",
        "feature_importance_combined = pd.concat(feature_importance_results, ignore_index=True)\n",
        "\n",
        "# Save the feature importance results to a file\n",
        "feature_importance_combined.to_csv(f'stacking_42_heating_feature_importance_experiment_{experiment}.csv', index=False)\n",
        "\n",
        "print(f\"Feature importance for seed 42 and experiment {experiment} has been successfully saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S985yyfqDINU"
      },
      "source": [
        "### Residual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2417d00-3fac-4960-ec03-d7fc3d9dd02a",
        "id": "KraZ8ZshDINU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed fold 1.\n",
            "Processed fold 2.\n",
            "Processed fold 3.\n",
            "Processed fold 4.\n",
            "Processed fold 5.\n",
            "All folds processed and merged data saved successfully to stacking_42_heating_residual.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "experiment = 2  # Experiment number\n",
        "merged_data = []  # Placeholder for storing final merged data\n",
        "\n",
        "# Load the best model parameters for each fold\n",
        "best_params_df = pd.read_csv('stacking_42_heating_hyperparameter_results_detailed.csv')\n",
        "\n",
        "# Loop through each fold\n",
        "for fold in range(1, 6):\n",
        "    # Load training data\n",
        "    train_data = pd.read_csv(f'fold{fold}_heating_filtered_train_42_data.csv')\n",
        "    X_train = train_data.drop(columns=['WH_RTU_Total'])\n",
        "    y_train = train_data['WH_RTU_Total']\n",
        "\n",
        "    # Get the best parameters for the current fold\n",
        "    best_params = best_params_df.loc[best_params_df['Fold'] == fold, 'Best Params'].iloc[0]\n",
        "    best_params_dict = eval(best_params)\n",
        "\n",
        "    # Define base models and Stacking Regressor with best parameters\n",
        "    base_models = [\n",
        "        ('rf', RandomForestRegressor(random_state=100,\n",
        "                                     n_estimators=best_params_dict['rf__n_estimators'],\n",
        "                                     max_depth=best_params_dict['rf__max_depth'])),\n",
        "        ('gb', GradientBoostingRegressor(random_state=100,\n",
        "                                         learning_rate=best_params_dict['gb__learning_rate'],\n",
        "                                         max_depth=best_params_dict['gb__max_depth']))\n",
        "    ]\n",
        "    meta_model = LinearRegression(fit_intercept=best_params_dict['final_estimator__fit_intercept'])\n",
        "    best_stacking = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "    # Train the Stacking Regressor\n",
        "    best_stacking.fit(X_train, y_train)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(f'fold{fold}_heating_filtered_test_42_data.csv')\n",
        "    test_data['Label'] = 'heating'  # Dynamically add label based on dataset type\n",
        "\n",
        "    # Split features and target\n",
        "    X_test = test_data.drop(columns=['WH_RTU_Total', 'Label'])\n",
        "    y_test = test_data['WH_RTU_Total']\n",
        "\n",
        "    # Predict and calculate residuals\n",
        "    y_pred = best_stacking.predict(X_test)\n",
        "    test_data = test_data.reset_index(drop=True)  # Reset test_data index\n",
        "    y_pred_series = pd.Series(y_pred, index=test_data.index)  # Align y_pred with test_data index\n",
        "    test_data['Predicted'] = y_pred_series\n",
        "    test_data['Residual'] = test_data['WH_RTU_Total'] - test_data['Predicted']\n",
        "\n",
        "    # Select and rename required columns\n",
        "    if 'T_out' in test_data.columns and 'RH_out' in test_data.columns:\n",
        "        selected_data = test_data[['WH_RTU_Total', 'Predicted', 'Residual', 'T_out', 'RH_out', 'Label']].rename(columns={\n",
        "            'WH_RTU_Total': 'Actual'\n",
        "        })\n",
        "    else:\n",
        "        missing_columns = [col for col in ['T_out', 'RH_out'] if col not in test_data.columns]\n",
        "        raise ValueError(f\"{', '.join(missing_columns)} is/are missing in test data for fold {fold}\")\n",
        "\n",
        "    # Add fold, seed, and model columns\n",
        "    selected_data['Fold'] = fold\n",
        "    selected_data['Seed'] = 42\n",
        "    selected_data['Model'] = 'StackingRegressor'\n",
        "    selected_data['Experiment'] = experiment\n",
        "\n",
        "    # Append to final merged data\n",
        "    merged_data.append(selected_data)\n",
        "    print(f\"Processed fold {fold}.\")\n",
        "\n",
        "# Combine all folds into a single DataFrame\n",
        "merged_final_data = pd.concat(merged_data, ignore_index=True)\n",
        "\n",
        "# Save the merged data\n",
        "output_file = 'stacking_42_heating_residual.csv'\n",
        "merged_final_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"All folds processed and merged data saved successfully to {output_file}.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}